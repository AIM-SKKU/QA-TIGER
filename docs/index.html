<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Question-Aware Gaussian Experts for Audio-Visual Question Answering"/>
  <meta property="og:description" content="QA-TIGER introduces a question-aware Gaussian expert model for audio-visual question answering, enhancing temporal alignment and multimodal reasoning for more accurate predictions."/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Question-Aware Gaussian Experts for Audio-Visual Question Answering">
  <meta name="twitter:description" content="QA-TIGER introduces a question-aware Gaussian expert model for audio-visual question answering, enhancing temporal alignment and multimodal reasoning for more accurate predictions.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Question-Aware Gaussian Experts for Audio-Visual Question Answering</title>
  <link rel="icon" type="image/x-icon" href="static/images/logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Question-Aware Gaussian Experts for Audio-Visual Question Answering</h1>
            <div class="is-size-5 publication-authors">
              <div style="color:#A40000; font-size: 36px; font-weight: bold;">
                <b>CVPR 2025</b>
              </div>
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://sites.google.com/view/redleafkim" target="_blank">Hongyeob Kim</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="https://github.com/nanacoco419" target="_blank">Inyoung Jung</a><sup>1*</sup>,</span>
                  <span class="author-block">
                    <a href="https://github.com/dayoonsuh" target="_blank">Dayoon Suh</a><sup>2</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://github.com/zhangyj66" target="_blank">Youjia Zhang</a><sup>1</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://sites.google.com/view/sangmin-lee/home" target="_blank">Sangmin Lee</a><sup>1</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://www.csehong.com/" target="_blank">Sungeun Hong</a><sup>1†</sup>
                  </span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Sungkyunkwan University</span>
                    <span class="author-block"><sup>2</sup>Purdue University</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                        </a>
                      </span>
                      <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2503.04459" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                        </a>
                      </span>
                      <!-- Video Link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon"><i class="fab fa-youtube"></i></span>
                        <span>Video</span>
                        </a>
                      </span>
                      <!-- Poster Link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon"><i class="fas fa-image"></i></span>
                        <span>Poster</span>
                        </a>
                      </span>
                      <!-- Github link -->
                      <span class="link-block">
                        <a href="https://github.com/AIM-SKKU/QA-TIGER" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                        </a>
                    </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="container has-text-centered">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/videos/QA-TIGER.mp4"
          type="video/mp4">
        </video>
        <!-- <img src="static/images/teaser1.png" alt="QA-TIGER Teaser Image">
        <img src="static/images/teaser2.png" alt="QA-TIGER Teaser Image"> -->
        <h3 class="subtitle">Comparison with Other Temporal Sampling Methods in AVQA</h3> 
        <h2 class="subtitle has-text-justified">
          This video presents the core idea of <b>Question-Aware Temporal Integration Gaussian Experts for Reasoning (QA-TIGER)</b>, our novel framework for 
          <b>Audio-Visual Question Answering (AVQA)</b>. It highlights key differences between QA-TIGER and traditional temporal sampling methods in AVQA.
          Unlike existing approaches that rely on <b>discrete</b> frame selection methods—such as uniform sampling or Top-K selection—QA-TIGER explicitly
          integrates question information and models <b>continuous</b> temporal dynamics 
          using a Gaussian-based weighting mechanism. By leveraging a <b>Mixture of Experts (MoE)</b>, 
          it adaptively captures question-relevant audio-visual cues and improves temporal alignment.
          The comparison demonstrates how QA-TIGER outperforms traditional uniform sampling 
          and Top-K frame selection, leading to more accurate and context-aware predictions.
        </h2>
      </div>
    </div>
  </div>
</section>
<!-- End teaser -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Audio-Visual Question Answering (AVQA) requires not only question-based multimodal reasoning but also precise temporal grounding to capture subtle dynamics for accurate prediction. 
            However, existing methods mainly use question information implicitly, limiting focus on question-specific details. 
            Furthermore, most studies rely on uniform frame sampling, which can miss key question-relevant frames. Although recent Top-K frame selection methods aim to address this, their discrete nature still overlooks fine-grained temporal details. 
            This paper proposes QA-TIGER, a novel framework that explicitly incorporates question information and models continuous temporal dynamics. 
            Our key idea is to use Gaussian-based modeling to adaptively focus on both consecutive and non-consecutive frames based on the question, while explicitly injecting question information and applying progressive refinement. 
            We leverage a Mixture of Experts (MoE) to flexibly implement multiple Gaussian models, activating temporal experts specifically tailored to the question. 
            Extensive experiments on multiple AVQA benchmarks show that QA-TIGER consistently achieves state-of-the-art performance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
    <div class="container has-text-centered">
      <h2 class="title">Overview</h2>
      <img src="static/images/main_figure.png" alt="QA-TIGER main figure">
      <h3 class="subtitle">Overview of <b>Q</b>uestion-<b>A</b>ware <b>T</b>emporal <b>I</b>ntegration <b>G</b>aussian <b>E</b>xperts for <b>R</b>easoning</h3> 
      <div class="content has-text-justified">
        <p>
          QA-TIGER first extracts visual features from video segments using a pretrained <b>CLIP image encoder</b> and audio features via a <b>VGGish model</b>,
          while the input question is encoded with <b>CLIP text encoder</b>.
          It then embeds the question context into both modalities through a <b>question-aware fusion module</b> that applies self- and cross-attention,
          generating enriched, query-aligned representations.
          Next, a <b>temporal integration module</b> leverages a <b>Mixture-of-Experts (MoE) framework</b> where multiple <b>Gaussian distributions</b> are generated
          to capture both consecutive and non-consecutive temporal dependencies.
          <b>Adaptive weights</b>, computed via a <b>routing mechanism</b>,
          allow the model to emphasize segments most relevant to the query.
          Finally, the fused audio-visual features are combined using a <b>question-guided reasoning module</b>
          and passed through a <b>linear classifier</b> with softmax to produce the final answer prediction.
        </p>
    </div>
    </div>
    </div>
  </div>
</section>


<style>
  @media screen and (min-width: 960px) {
    .table-container {
      overflow-x: auto;
      -webkit-overflow-scrolling: touch;
    }
  }
  /* 테이블 전체의 경계 합치기 */
  .table-container table {
    border-collapse: collapse;
    /* 필요하다면 상단/하단 전체 테두리 추가 */
    border-top: 2px solid #b8b8b8;
    border-bottom: 2px solid #b8b8b8;
  }
  /* 기존 스타일은 그대로 두고 추가 */
  .table-container table th,
  .table-container table td {
    text-align: center;
    vertical-align: middle;
    white-space: nowrap;
  }
  
</style>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h2 class="title">QA-TIGER Quantitative Results</h2>
      <div class="container is-max-desktop">
      <p>
        <div class="content has-text-justified">
        To evaluate our model’s effectiveness, we compared <b>QA-TIGER</b>
        to existing AVQA methods on Audio (<b>A-QA</b>), Visual (<b>V-QA</b>), and Audio-Visual (<b>AV-QA</b>) tasks,
        considering both question types and overall averages. <b>QA-TIGER</b> was trained on the <b>MUSIC-AVQA</b>
        training set and validated using its validation set for testing both <b>MUSIC-AVQA</b> and <b>MUSIC-AVQA-R</b>.
        For <b>MUSIC-AVQA-v2.0</b>, <b>QA-TIGER</b> was trained on both the <b>balanced</b> and <b>biased</b> training sets and tested accordingly.
        </div>
      </p>
      <br>
      <div class="table-container">
        <table class="table is-striped is-hoverable is-fullwidth">
            <thead>
                <tr>
                    <th rowspan="2" style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;">Method</th>
                    <th colspan="3" style="border-right: 1px solid #b8b8b8; text-align: center;">Audio QA</th>
                    <th colspan="3" style="border-right: 1px solid #b8b8b8; text-align: center;">Visual QA</th>
                    <th colspan="6" style="border-right: 1px solid #b8b8b8; text-align: center;">Audio-Visual QA</th>
                    <th rowspan="2" style="text-align: center; vertical-align: middle;">Avg</th>
                </tr>
                <tr>
                    <th style="text-align: center;">Count</th>
                    <th style="text-align: center;">Comp</th>
                    <th style="border-right: 1px solid #b8b8b8; text-align: center;">Avg</th>
                    <th style="text-align: center;">Count</th>
                    <th style="text-align: center;">Local</th>
                    <th style="border-right: 1px solid #b8b8b8; text-align: center;">Avg</th>
                    <th style="text-align: center;">Exist</th>
                    <th style="text-align: center;">Count</th>
                    <th style="text-align: center;">Local</th>
                    <th style="text-align: center;">Comp</th>
                    <th style="text-align: center;">Temp</th>
                    <th style="border-right: 1px solid #b8b8b8; text-align: center;">Avg</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="https://ieeexplore.ieee.org/abstract/document/9145807/">FCNLSTM</a></td>
                    <td style="text-align: center;">70.45</td>
                    <td style="text-align: center;">66.22</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">68.88</td>
                    <td style="text-align: center;">63.89</td>
                    <td style="text-align: center;">46.74</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">55.21</td>
                    <td style="text-align: center;">82.01</td>
                    <td style="text-align: center;">59.34</td>
                    <td style="text-align: center;">46.28</td>
                    <td style="text-align: center;">62.15</td>
                    <td style="text-align: center;">47.33</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">60.06</td>
                    <td style="text-align: center; vertical-align: middle;">60.34</td>
                </tr>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="https://ieeexplore.ieee.org/abstract/document/9251129">BiLSTM</a></td>
                    <td style="text-align: center;">70.35</td>
                    <td style="text-align: center;">47.92</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">62.05</td>
                    <td style="text-align: center;">64.64</td>
                    <td style="text-align: center;">64.33</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">64.48</td>
                    <td style="text-align: center;">78.39</td>
                    <td style="text-align: center;">56.91</td>
                    <td style="text-align: center;">45.85</td>
                    <td style="text-align: center;">53.09</td>
                    <td style="text-align: center;">49.76</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">57.10</td>
                    <td style="text-align: center; vertical-align: middle;">59.92</td>
                </tr>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="https://proceedings.neurips.cc/paper/2016/hash/9dcb88e0137649590b755372b040afad-Abstract.html">HCAttn</a></td>
                    <td style="text-align: center;">70.25</td>
                    <td style="text-align: center;">54.91</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">64.57</td>
                    <td style="text-align: center;">64.05</td>
                    <td style="text-align: center;">66.37</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">65.22</td>
                    <td style="text-align: center;">79.10</td>
                    <td style="text-align: center;">59.97</td>
                    <td style="text-align: center;">49.51</td>
                    <td style="text-align: center;">55.25</td>
                    <td style="text-align: center;">56.43</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">60.19</td>
                    <td style="text-align: center; vertical-align: middle;">62.30</td>
                </tr>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Deep_Modular_Co-Attention_Networks_for_Visual_Question_Answering_CVPR_2019_paper.html">MCAN</a></td>
                    <td style="text-align: center;">77.50</td>
                    <td style="text-align: center;">55.24</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">69.25</td>
                    <td style="text-align: center;">71.56</td>
                    <td style="text-align: center;">70.93</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">71.24</td>
                    <td style="text-align: center;">80.40</td>
                    <td style="text-align: center;">64.91</td>
                    <td style="text-align: center;">54.48</td>
                    <td style="text-align: center;">57.22</td>
                    <td style="text-align: center;">47.57</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">61.58</td>
                    <td style="text-align: center; vertical-align: middle;">65.49</td>
                </tr>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/4887">PSAC</a></td>
                    <td style="text-align: center;">75.64</td>
                    <td style="text-align: center;">66.06</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">72.09</td>
                    <td style="text-align: center;">68.64</td>
                    <td style="text-align: center;">69.79</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">69.22</td>
                    <td style="text-align: center;">77.59</td>
                    <td style="text-align: center;">63.42</td>
                    <td style="text-align: center;">55.02</td>
                    <td style="text-align: center;">61.17</td>
                    <td style="text-align: center;">59.47</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">63.52</td>
                    <td style="text-align: center; vertical-align: middle;">66.54</td>
                </tr>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Fan_Heterogeneous_Memory_Enhanced_Multimodal_Attention_Model_for_Video_Question_Answering_CVPR_2019_paper.html">HME</a></td>
                    <td style="text-align: center;">74.76</td>
                    <td style="text-align: center;">63.56</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">70.61</td>
                    <td style="text-align: center;">67.97</td>
                    <td style="text-align: center;">69.46</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">68.76</td>
                    <td style="text-align: center;">80.30</td>
                    <td style="text-align: center;">63.19</td>
                    <td style="text-align: center;">53.18</td>
                    <td style="text-align: center;">62.69</td>
                    <td style="text-align: center;">59.83</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">64.05</td>
                    <td style="text-align: center; vertical-align: middle;">66.45</td>
                </tr>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Le_Hierarchical_Conditional_Relation_Networks_for_Video_Question_Answering_CVPR_2020_paper.html">HCRN</a></td>
                    <td style="text-align: center;">68.59</td>
                    <td style="text-align: center;">50.92</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">62.05</td>
                    <td style="text-align: center;">64.39</td>
                    <td style="text-align: center;">61.81</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">63.08</td>
                    <td style="text-align: center;">54.47</td>
                    <td style="text-align: center;">53.38</td>
                    <td style="text-align: center;">41.53</td>
                    <td style="text-align: center;">52.11</td>
                    <td style="text-align: center;">47.69</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">50.26</td>
                    <td style="text-align: center; vertical-align: middle;">55.73</td>
                </tr>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Schwartz_A_Simple_Baseline_for_Audio-Visual_Scene-Aware_Dialog_CVPR_2019_paper.html">AVSD</a></td>
                    <td style="text-align: center;">72.41</td>
                    <td style="text-align: center;">61.90</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">68.52</td>
                    <td style="text-align: center;">67.39</td>
                    <td style="text-align: center;">74.19</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">70.83</td>
                    <td style="text-align: center;">81.61</td>
                    <td style="text-align: center;">63.89</td>
                    <td style="text-align: center;">58.79</td>
                    <td style="text-align: center;">61.52</td>
                    <td style="text-align: center;">61.41</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">65.49</td>
                    <td style="text-align: center; vertical-align: middle;">67.44</td>
                </tr>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content/ICCV2021/html/Yun_Pano-AVQA_Grounded_Audio-Visual_Question_Answering_on_360deg_Videos_ICCV_2021_paper.html">Pano-AVQA</a></td>
                    <td style="text-align: center;">74.36</td>
                    <td style="text-align: center;">64.56</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">70.73</td>
                    <td style="text-align: center;">69.39</td>
                    <td style="text-align: center;">75.65</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">72.56</td>
                    <td style="text-align: center;">81.21</td>
                    <td style="text-align: center;">64.91</td>
                    <td style="text-align: center;">59.33</td>
                    <td style="text-align: center;">64.22</td>
                    <td style="text-align: center;">63.23</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">66.64</td>
                    <td style="text-align: center; vertical-align: middle;">68.93</td>
                </tr>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content/CVPR2022/html/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.html">ST-AVQA</a></td>
                    <td style="text-align: center;">78.18</td>
                    <td style="text-align: center;">67.05</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">74.06</td>
                    <td style="text-align: center;">71.56</td>
                    <td style="text-align: center;">76.38</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">74.00</td>
                    <td style="text-align: center;">81.81</td>
                    <td style="text-align: center;">70.80</td>
                    <td style="text-align: center;">64.51</td>
                    <td style="text-align: center;"><u>66.01</u></td>
                    <td style="text-align: center;">63.23</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">69.54</td>
                    <td style="text-align: center; vertical-align: middle;">71.52</td>
                </tr>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/26527">COCA</a></td>
                    <td style="text-align: center;">79.35</td>
                    <td style="text-align: center;">67.68</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">75.42</td>
                    <td style="text-align: center;">75.10</td>
                    <td style="text-align: center;">75.43</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">75.23</td>
                    <td style="text-align: center;"><strong>83.50</strong></td>
                    <td style="text-align: center;">66.63</td>
                    <td style="text-align: center;">69.72</td>
                    <td style="text-align: center;">64.12</td>
                    <td style="text-align: center;">65.57</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">69.96</td>
                    <td style="text-align: center;">72.33</td>
                </tr>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612293">PSTP-Net</a></td>
                    <td style="text-align: center;">73.97</td>
                    <td style="text-align: center;">65.59</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">70.91</td>
                    <td style="text-align: center;">77.15</td>
                    <td style="text-align: center;">77.36</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">77.26</td>
                    <td style="text-align: center;">76.18</td>
                    <td style="text-align: center;">72.23</td>
                    <td style="text-align: center;">71.80</td>
                    <td style="text-align: center;"><strong>71.79</strong></td>
                    <td style="text-align: center;">69.00</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">72.57</td>
                    <td style="text-align: center;">73.52</td>
                </tr>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content/CVPR2023/html/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.html">LAVISH</a></td>
                    <td style="text-align: center;">82.09</td>
                    <td style="text-align: center;">65.56</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">75.97</td>
                    <td style="text-align: center;">78.98</td>
                    <td style="text-align: center;">81.43</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">80.22</td>
                    <td style="text-align: center;">81.71</td>
                    <td style="text-align: center;">75.51</td>
                    <td style="text-align: center;">66.13</td>
                    <td style="text-align: center;">63.77</td>
                    <td style="text-align: center;">67.96</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">71.26</td>
                    <td style="text-align: center;">74.46</td>
                </tr>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/28116">APL</a></td>
                    <td style="text-align: center;">82.40</td>
                    <td style="text-align: center;"><strong>70.71</strong></td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;"><u>78.09</u></td>
                    <td style="text-align: center;">76.52</td>
                    <td style="text-align: center;">82.74</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">79.69</td>
                    <td style="text-align: center;">82.99</td>
                    <td style="text-align: center;">73.29</td>
                    <td style="text-align: center;">66.68</td>
                    <td style="text-align: center;">64.76</td>
                    <td style="text-align: center;">65.95</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">0.96</td>
                    <td style="text-align: center;">74.53</td>
                </tr>
                <tr>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="https://dl.acm.org/doi/abs/10.1145/3664647.3680803">TSPM</a></td>
                    <td style="text-align: center;"><u>84.07</u></td>
                    <td style="text-align: center;">64.65</td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;">76.91</td>
                    <td style="text-align: center;"><u>82.29</u></td>
                    <td style="text-align: center;"><u>84.90</u></td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;"><u>83.61</u></td>
                    <td style="text-align: center;">82.19</td>
                    <td style="text-align: center;"><u>76.21</u></td>
                    <td style="text-align: center;"><u>71.85</u></td>
                    <td style="text-align: center;">65.76</td>
                    <td style="text-align: center;"><strong>71.17</strong></td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;"><u>73.51</u></td>
                    <td style="text-align: center;"><u>76.79</u></td>
                </tr>
                <tr class="graycell">
                    <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;white-space: nowrap"><strong>QA-TIGER</strong></td>
                    <td style="text-align: center;"><strong>84.86</strong></td>
                    <td style="text-align: center;"><u>67.85</u></td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;"><strong>78.58</strong></td>
                    <td style="text-align: center;"><strong>83.96</strong></td>
                    <td style="text-align: center;"><strong>86.29</strong></td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;"><strong>85.14</strong></td>
                    <td style="text-align: center;"><u>83.10</u></td>
                    <td style="text-align: center;"><strong>78.58</strong></td>
                    <td style="text-align: center;"><strong>72.50</strong></td>
                    <td style="text-align: center;">63.94</td>
                    <td style="text-align: center;"><u>69.59</u></td>
                    <td style="border-right: 1px solid #b8b8b8; text-align: center;"><strong>73.74</strong></td>
                    <td style="text-align: center;"><strong>77.62</strong></td>
                </tr>
            </tbody>
        </table>
        </div>
        <h3 class="subtitle"><b>MUSIC-AVQA</b></h3> 
        <div class="content has-text-justified">
          <p>
          <b>QA-TIGER</b> achieves the highest performance among all models, with an overall accuracy (77.62%), surpassing the previous best method, <b>TSPM</b> (76.79%). 
          Notably, our model shows strong performance in complex reasoning tasks, achieving 78.58% and 72.50% in <b>AV-Counting</b> and <b>AV-Local</b> categories respectively, surpassing previous methods.
        </p>
        <br>
        <br>
        </div>
      </div>

      <div class="container is-max-desktop">
      <div class="table-container">
        <table class="table is-striped is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th rowspan="3" style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;">Method</th>
              <th colspan="4" style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;">Audio QA</th>
              <th colspan="4" style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;">Visual QA</th>
              <th colspan="10" style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;">Audio-Visual QA</th>
              <th rowspan="3" style="text-align: center; vertical-align: middle;">Avg</th>
            </tr>
            <tr>
              <th colspan="2" style="text-align: center; vertical-align: middle;">Count</th>
              <th colspan="2" style="text-align: center; vertical-align: middle;border-right: 1px solid #b8b8b8;">Comp</th>
              <th colspan="2" style="text-align: center; vertical-align: middle;">Count</th>
              <th colspan="2" style="text-align: center; vertical-align: middle;border-right: 1px solid #b8b8b8;">Local</th>
              <th colspan="2" style="text-align: center; vertical-align: middle;">Exist</th>
              <th colspan="2" style="text-align: center; vertical-align: middle;">Count</th>
              <th colspan="2" style="text-align: center; vertical-align: middle;">Local</th>
              <th colspan="2" style="text-align: center; vertical-align: middle;">Comp</th>
              <th colspan="2" style="text-align: center; vertical-align: middle;border-right: 1px solid #b8b8b8;">Temp</th>
            </tr>
            <tr>
              <th style="text-align: center; vertical-align: middle;">H</th>
              <th style="text-align: center; vertical-align: middle;">T</th>
              <th style="text-align: center; vertical-align: middle;">H</th>
              <th style="text-align: center; vertical-align: middle; border-right: 1px solid #b8b8b8;">T</th>
              <th style="text-align: center; vertical-align: middle;">H</th>
              <th style="text-align: center; vertical-align: middle;">T</th>
              <th style="text-align: center; vertical-align: middle;">H</th>
              <th style="text-align: center; vertical-align: middle;border-right: 1px solid #b8b8b8;">T</th>
              <th style="text-align: center; vertical-align: middle;">H</th>
              <th style="text-align: center; vertical-align: middle;">T</th>
              <th style="text-align: center; vertical-align: middle;">H</th>
              <th style="text-align: center; vertical-align: middle;">T</th>
              <th style="text-align: center; vertical-align: middle;">H</th>
              <th style="text-align: center; vertical-align: middle;">T</th>
              <th style="text-align: center; vertical-align: middle;">H</th>
              <th style="text-align: center; vertical-align: middle;">T</th>
              <th style="text-align: center; vertical-align: middle;">H</th>
              <th style="text-align: center; vertical-align: middle;border-right: 1px solid #b8b8b8;">T</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="https://ieeexplore.ieee.org/abstract/document/9145807/">FCNLSTM</a></td>
              <td>66.23</td>
              <td>36.48</td>
              <td>64.78</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">51.24</td>
              <td>61.75</td>
              <td>5.31</td>
              <td>54.86</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">51.06</td>
              <td>64.76</td>
              <td>78.52</td>
              <td>62.69</td>
              <td>7.23</td>
              <td><u>46.66</u></td>
              <td>57.30</td>
              <td>43.13</td>
              <td>71.67</td>
              <td>37.02</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">30.78</td>
              <td>54.12</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="https://ieeexplore.ieee.org/abstract/document/9251129">BiLSTM</a></td>
              <td>73.68</td>
              <td>46.32</td>
              <td>21.51</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;"><strong>77.58</strong></td>
              <td>64.30</td>
              <td>0.00</td>
              <td>53.92</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">42.01</td>
              <td><strong>87.51</strong></td>
              <td>21.14</td>
              <td>62.85</td>
              <td>2.18</td>
              <td>35.16</td>
              <td>43.75</td>
              <td>27.61</td>
              <td><u>74.38</u></td>
              <td>17.58</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">31.32</td>
              <td>48.84</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="https://proceedings.neurips.cc/paper/2016/hash/9dcb88e0137649590b755372b040afad-Abstract.html">HCAttn</a></td>
              <td>61.67</td>
              <td>41.63</td>
              <td>59.09</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">47.14</td>
              <td>56.52</td>
              <td>9.20</td>
              <td>67.01</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">53.16</td>
              <td>66.57</td>
              <td>61.13</td>
              <td>59.53</td>
              <td>12.48</td>
              <td>37.05</td>
              <td>42.48</td>
              <td>48.81</td>
              <td>60.12</td>
              <td>33.82</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">39.26</td>
              <td>51.90</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Deep_Modular_Co-Attention_Networks_for_Visual_Question_Answering_CVPR_2019_paper.html">MCAN</a></td>
              <td>75.02</td>
              <td>60.16</td>
              <td>58.89</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">50.09</td>
              <td>64.58</td>
              <td>26.69</td>
              <td>66.48</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">62.25</td>
              <td>51.29</td>
              <td>67.29</td>
              <td>64.76</td>
              <td>25.28</td>
              <td>46.11</td>
              <td>61.61</td>
              <td>50.57</td>
              <td>52.40</td>
              <td>34.64</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;"><u>58.05</u></td>
              <td>57.27</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/4887">PSAC</a></td>
              <td>53.01</td>
              <td>56.68</td>
              <td>57.41</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">48.12</td>
              <td>49.55</td>
              <td>26.43</td>
              <td>72.96</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">60.69</td>
              <td>50.56</td>
              <td>55.54</td>
              <td>56.70</td>
              <td>19.58</td>
              <td>41.98</td>
              <td>52.30</td>
              <td>38.13</td>
              <td>58.92</td>
              <td>26.68</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">46.24</td>
              <td>50.45</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Fan_Heterogeneous_Memory_Enhanced_Multimodal_Attention_Model_for_Video_Question_Answering_CVPR_2019_paper.html">HME</a></td>
              <td>62.60</td>
              <td>53.95</td>
              <td>54.97</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">58.29</td>
              <td>50.95</td>
              <td>16.46</td>
              <td>73.25</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">58.60</td>
              <td>65.74</td>
              <td>66.49</td>
              <td>63.18</td>
              <td>17.18</td>
              <td>33.79</td>
              <td>46.03</td>
              <td>53.20</td>
              <td>69.57</td>
              <td>33.95</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">41.57</td>
              <td>53.66</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Le_Hierarchical_Conditional_Relation_Networks_for_Video_Question_Answering_CVPR_2020_paper.html">HCRN</a></td>
              <td>55.53</td>
              <td>53.31</td>
              <td>47.17</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">32.44</td>
              <td>41.87</td>
              <td>23.55</td>
              <td>39.40</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">51.27</td>
              <td>41.81</td>
              <td>65.45</td>
              <td>54.58</td>
              <td>19.57</td>
              <td>36.62</td>
              <td>42.72</td>
              <td>33.33</td>
              <td>36.87</td>
              <td style="text-align: center; vertical-align: middle;"><u>40.47</u></td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">44.13</td>
              <td>43.92</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Schwartz_A_Simple_Baseline_for_Audio-Visual_Scene-Aware_Dialog_CVPR_2019_paper.html">AVSD</a></td>
              <td>54.00</td>
              <td>47.84</td>
              <td>60.61</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">47.79</td>
              <td>60.34</td>
              <td>10.07</td>
              <td>74.78</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">61.43</td>
              <td>66.28</td>
              <td>61.98</td>
              <td>46.21</td>
              <td>8.06</td>
              <td>33.00</td>
              <td>40.35</td>
              <td>51.98</td>
              <td>66.00</td>
              <td>40.14</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">41.52</td>
              <td>52.33</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle; white-space: nowrap;"><a href="http://openaccess.thecvf.com/content/ICCV2021/html/Yun_Pano-AVQA_Grounded_Audio-Visual_Question_Answering_on_360deg_Videos_ICCV_2021_paper.html">Pano-AVQA</a></td>
              <td>50.57</td>
              <td>43.45</td>
              <td>50.78</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">44.93</td>
              <td>47.28</td>
              <td>15.50</td>
              <td>67.19</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">65.51</td>
              <td>52.37</td>
              <td>22.04</td>
              <td>52.21</td>
              <td>21.52</td>
              <td>44.35</td>
              <td style="text-align: center; vertical-align: middle;"><u>61.69</u></td>
              <td>45.61</td>
              <td>40.49</td>
              <td>35.00</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">49.33</td>
              <td>47.40</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content/CVPR2022/html/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.html">ST-AVQA</a></td>
              <td>56.40</td>
              <td>41.48</td>
              <td>62.28</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">57.59</td>
              <td>59.86</td>
              <td>12.94</td>
              <td>63.31</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">54.00</td>
              <td>73.35</td>
              <td>77.26</td>
              <td>48.31</td>
              <td>8.41</td>
              <td>35.35</td>
              <td>40.49</td>
              <td style="text-align: center; vertical-align: middle;"><u>53.30</u></td>
              <td>62.44</td>
              <td>40.25</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">38.15</td>
              <td>52.80</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content/CVPR2023/html/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.html">LAVISH</a></td>
              <td>61.73</td>
              <td>43.99</td>
              <td>65.06</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;"><u>60.38</u></td>
              <td>65.53</td>
              <td>11.13</td>
              <td>70.21</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">64.73</td>
              <td><u>77.83</u></td>
              <td><u>79.46</u></td>
              <td>49.88</td>
              <td>14.87</td>
              <td>41.76</td>
              <td>41.20</td>
              <td><strong>59.26</strong></td>
              <td>65.10</td>
              <td><strong>41.84</strong></td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">46.26</td>
              <td>57.63</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="https://dl.acm.org/doi/abs/10.1145/3664647.3680803">TSPM</a></td>
              <td><u>81.65</u></td>
              <td><u>71.80</u></td>
              <td><u>67.66</u></td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">49.56</td>
              <td><u>78.29</u></td>
              <td><u>47.56</u></td>
              <td><u>80.58</u></td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;"><u>73.18</u></td>
              <td>69.15</td>
              <td><strong>82.79</strong></td>
              <td><strong>77.09</strong></td>
              <td><strong>38.64</strong></td>
              <td>42.24</td>
              <td>57.37</td>
              <td>52.07</td>
              <td>68.86</td>
              <td>39.23</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">49.36</td>
              <td><u>66.30</u></td>
            </tr>
            <tr class="graycell">
              <td style="border-right: 1px solid #b8b8b8; text-align: center;"><strong>QA-TIGER</strong></td>
              <td><strong>82.67</strong></td>
              <td><strong>75.82</strong></td>
              <td><strong>71.75</strong></td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;">43.11</td>
              <td><strong>81.30</strong></td>
              <td><strong>54.59</strong></td>
              <td><strong>84.76</strong></td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;"><strong>75.59</strong></td>
              <td>72.84</td>
              <td>78.56</td>
              <td style="text-align: center; vertical-align: middle;"><u>76.70</u></td>
              <td style="text-align: center; vertical-align: middle;"><u>33.55</u></td>
              <td><strong>48.22</strong></td>
              <td><strong>64.65</strong></td>
              <td>37.55</td>
              <td><strong>80.47</strong></td>
              <td>36.85</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center;"><strong>62.96</strong></td>
              <td><strong>67.99</strong></td>
            </tr>
          </tbody>
        </table>        
    </div>
    <h3 class="subtitle"><b>MUSIC-AVQA-R</b></h3> 
    <div class="content has-text-justified">
      <p>
        Our method achieves 67.99% overall accuracy on the <b>MUSIC-AVQA-R</b> dataset across diverse question types,
        without the need for explicit bias handling techniques. This balanced performance highlights <b>QA-TIGER</b>’s
        strong temporal modeling and question-aware feature extraction capabilities.
    </p>
    <br>
    <br>
    </div>
    </div>

    <div class="container is-max-desktop">
      <div class="table-container">
        <style>
          .table-container table tbody + tbody {
            border-top: 2px solid #dbdbdb;
          }
          .table-container table th:first-child,
          .table-container table td:first-child {
            border-right: 1px solid #b8b8b8;
          }
        </style>
        <table class="table is-striped is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Test</th>
              <th style=" border-right: 1px solid #b8b8b8;">Training</th>
              <th style=" border-right: 1px solid #b8b8b8;">Method</th>
              <th>A-QA</th>
              <th>V-QA</th>
              <th>AV-QA</th>
              <th>Avg</th>
            </tr>
          </thead>
          <tbody>
            <!-- Group (a) Bias -->
            <tr>
              <td rowspan="8">(a) Bias</td>
              <td rowspan="3"; style="border-right: 1px solid #b8b8b8;">Bias</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content/CVPR2022/html/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.html">ST-AVQA</a></td>
              <td><u>76.86</u></td>
              <td>77.70</td>
              <td>69.59</td>
              <td>73.07</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content/CVPR2023/html/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.html">LAVISH</a></td>
              <td>76.73</td>
              <td><u>80.96</u></td>
              <td><u>70.80</u></td>
              <td><u>74.59</u></td>
            </tr>
            <tr>
              <td><strong>QA-TIGER</strong></td>
              <td><strong>79.13</strong></td>
              <td><strong>84.83</strong></td>
              <td><strong>72.37</strong></td>
              <td><strong>76.93</strong></td>
            </tr>
            <tr>
              <td rowspan="5">Balance</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content/CVPR2022/html/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.html">ST-AVQA</a></td>
              <td>76.18</td>
              <td>77.20</td>
              <td>67.96</td>
              <td>71.92</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content/CVPR2023/html/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.html">LAVISH</a></td>
              <td>75.56</td>
              <td>80.83</td>
              <td>69.27</td>
              <td>73.51</td>
            </tr>
            <tr>
              <td><a href="https://openaccess.thecvf.com/content/WACV2024/html/Liu_Tackling_Data_Bias_in_MUSIC-AVQA_Crafting_a_Balanced_Dataset_for_WACV_2024_paper.html">LAST</a></td>
              <td><u>77.10</u></td>
              <td>82.99</td>
              <td>70.86</td>
              <td>75.24</td>
            </tr>
            <tr>
              <td><a href="https://openaccess.thecvf.com/content/WACV2024/html/Liu_Tackling_Data_Bias_in_MUSIC-AVQA_Crafting_a_Balanced_Dataset_for_WACV_2024_paper.html">LAST-Att</a> </td>
              <td><strong>77.29</strong></td>
              <td><u>83.47</u></td>
              <td><u>71.05</u></td>
              <td><u>75.45</u></td>
            </tr>
            <tr>
              <td><strong>QA-TIGER</strong></td>
              <td>77.07</td>
              <td><strong>85.93</strong></td>
              <td><strong>71.20</strong></td>
              <td><strong>76.57</strong></td>
            </tr>
          </tbody>
          <tbody>
            <!-- Group (b) Balance -->
            <tr>
              <td rowspan="8">(b) Balance</td>
              <td rowspan="3"; style="border-right: 1px solid #b8b8b8;">Bias</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content/CVPR2022/html/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.html">ST-AVQA</a></td>
              <td><u>73.34</u></td>
              <td>76.82</td>
              <td>64.51</td>
              <td>69.40</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content/CVPR2023/html/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.html">LAVISH</a></td>
              <td>73.14</td>
              <td><u>79.70</u></td>
              <td><u>65.01</u></td>
              <td><u>70.39</u></td>
            </tr>
            <tr>
              <td><strong>QA-TIGER</strong></td>
              <td><strong>77.57</strong></td>
              <td><strong>84.84</strong></td>
              <td><strong>67.43</strong></td>
              <td><strong>73.91</strong></td>
            </tr>
            <tr>
              <td rowspan="5">Balance</td>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content/CVPR2022/html/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.html">ST-AVQA</a></td>
              <td>75.50</td>
              <td>77.67</td>
              <td>66.32</td>
              <td>71.02</td>
            </tr>
            <tr>
              <td style="border-right: 1px solid #b8b8b8; text-align: center; vertical-align: middle;"><a href="http://openaccess.thecvf.com/content/CVPR2023/html/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.html">LAVISH</a></td>
              <td>76.15</td>
              <td>81.32</td>
              <td>68.28</td>
              <td>73.18</td>
            </tr>
            <tr>
              <td><a href="https://openaccess.thecvf.com/content/WACV2024/html/Liu_Tackling_Data_Bias_in_MUSIC-AVQA_Crafting_a_Balanced_Dataset_for_WACV_2024_paper.html">LAST</a> </td>
              <td>78.08</td>
              <td>83.29</td>
              <td>69.72</td>
              <td>74.85</td>
            </tr>
            <tr>
              <td><a href="https://openaccess.thecvf.com/content/WACV2024/html/Liu_Tackling_Data_Bias_in_MUSIC-AVQA_Crafting_a_Balanced_Dataset_for_WACV_2024_paper.html">LAST-Att</a></td>
              <td><u>78.56</u></td>
              <td><u>84.07</u></td>
              <td><strong>70.30</strong></td>
              <td><u>75.44</u></td>
            </tr>
            <tr>
              <td><strong>QA-TIGER</strong></td>
              <td><strong>79.90</strong></td>
              <td><strong>86.95</strong></td>
              <td><u>70.22</u></td>
              <td><strong>76.43</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
      <h3 class="subtitle"><b>MUSIC-AVQA-v2.0</b></h3> 
      <div class="content has-text-justified">
        <p>
          <b>QA-TIGER</b> consistently outperforms existing models,
          like ST-AVQA, LAVISH, LAST(-Att), on the biased test set,
          regardless of the training set type (Table a). On the balanced test set (Table b),
          <b>QA-TIGER</b> trained on the balanced dataset achieves superior accuracy in both <b>A-QA</b> (79.90%) and <b>V-QA</b> (86.95%),
          surpassing LAST-Att in overall accuracy (75.44% vs. 76.43%). Notably, LAST-Att underperforms in <b>A-QA</b> despite incorporating an additional audio encoder,
          Audio Spectrogram Transformer. This highlights <b>QA-TIGER</b>’s robustness across diverse evaluation settings.
      </p>
      <br>
      <br>
      </div>
      </div>
      
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
    <div class="container has-text-centered">
      <h2 class="title">QA-TIGER Qualitative Results</h2>

      <div class="container is-max-desktop">
        <div class="content has-text-justified">
          <p>
            To evaluate whether <b>QA-TIGER</b> accurately identifies question-relevant temporal segments,
            we qualitatively analyzed its performance using <b>MUSIC-AVQA</b> benchmark.
            Notably, the audio Gaussian aligns more with <b>A-QA</b>,
            while the visual Gaussian is more prominent for <b>V-QA</b>;
            for <b>AV-QA</b>, both modalities exhibit similar Gaussian distributions,
            demonstrating the model’s adaptive focus based on the question type,
            with a detailed comparison using <b>uniform sampling</b> and <b>Top-K selection</b> provided in the supplementary material.
            <br>
            <br>
          </p>
        </div>
      </div>
      
      <!-- (a) Audio Question -->
      <div class="image-scroll-container" style="overflow-x: auto; overflow-y: hidden;">
        <div style="width: max-content;">
          <img src="static/images/qualitative_a.png" alt="qualitative-a">
        </div>
      </div>
      <h3 class="subtitle">(a) Audio Question</h3>
      <div class="container is-max-desktop">
        <div class="content has-text-justified">
          <p>
             <b>QA-TIGER</b> demonstrates its temporal reasoning capabilities
            by assigning high weights across the entire duration of the audio Gaussian when comparing the tuba
            and the clarinet—effectively capturing the complete temporal spans necessary
            for an accurate comparison—while the visual Gaussian focuses on frames where the instruments are actively played.
            <br>
            <br>
          </p>
        </div>
      </div>
      
      <!-- (b) Visual Question -->
      <div class="image-scroll-container" style="overflow-x: auto; overflow-y: hidden;">
        <div style="width: max-content;">
          <img src="static/images/qualitative_b.png" alt="qualitative-b">
        </div>
      </div>
      <h3 class="subtitle">(b) Visual Question</h3>
      <div class="container is-max-desktop">
        <div class="content has-text-justified">
          <p>
            The method effectively integrates both visual and audio modalities to accurately count the saxophones;
            the visual Gaussian assigns higher weights to frames where all five saxophonists are clearly visible from the front,
            whereas frames with less distinct views receive lower weights, and the audio Gaussian emphasizes moments
            when the sounds of all five saxophones overlap, ensuring that the model concentrates on the most critical visual and auditory cues.
            <br>
            <br>
          </p>
        </div>
      </div>
      
      <!-- (c) Audio-Visual Question -->
      <div class="image-scroll-container" style="overflow-x: auto; overflow-y: hidden;">
        <div style="width: max-content;">
          <img src="static/images/qualitative_c.png" alt="qualitative-c">
        </div>
      </div>
      <h3 class="subtitle">(c) Audio-Visual Question</h3>
      <div class="container is-max-desktop">
        <div class="content has-text-justified">
          <p>
            The visual Gaussian highlights frames in the early part of the video where individual instruments
            appear as well as those where all three instruments are shown together, and the audio Gaussian emphasizes
            the prominent sounds of the instruments. This complementary alignment enables <b>QA-TIGER</b>
            to effectively integrate visual and auditory cues, resulting in the accurate identification of all instruments.
          </p>
        </div>
      </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
    <div class="container has-text-centered">
      <h2 class="title">Question-Aware Attention Visualization</h2>

      <div class="container is-max-desktop">
        <div class="content has-text-justified">
          <p>
            To validate the effectiveness of our <b>question-aware fusion</b> module, we performed <b>word-level visualizations</b>.
            Additional supporting examples are provided in the <b>supplementary material</b>. 
            <br>
            <br>
          </p>
        </div>
      </div>
      
      <!-- (a) Audio Question -->
      <div class="image-scroll-container" style="overflow-x: auto; overflow-y: hidden;">
        <div style="width: max-content;">
          <img src="static/images/qa-attention.png" alt="qa-attention">
        </div>
      </div>
      <h3 class="subtitle">Audio-Visual Temporal Question</h3>
      <div class="container is-max-desktop">
        <div class="content has-text-justified">
          <p>
             <b>question-aware fusion</b> module focuses on “instrument,” “right,” “left,” and “louder” to identify spatial
             locations in <b>visual modality</b>. Meanwhile, it emphasizes
             “louder” to analyze sound intensities in <b>audio modality</b>.
             This complementary approach enables the model to tackle
             the question effectively.
            <br>
            <br>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>Will be updated</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>